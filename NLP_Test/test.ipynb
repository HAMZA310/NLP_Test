{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Natural Language Processing\n",
    "Here, we'll predict political party ('Conservative' or 'Labour') of a Member of Parliament (MP) of UK Parliament based on his/her tweets. The data (tweets of ~500 MPs since Aug 23- 2020) were extracted using V2 of Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de93eac73f51f1ea5a76eb2b77ceaf38",
     "grade": false,
     "grade_id": "cell-68c7629f85a0c398",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Useful in beautifying numpy arrays.\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "def pp(a, show_head=True): \n",
    "    '''\n",
    "    args: show_head -> if True print only first 5 rows.\n",
    "    return: None\n",
    "    '''\n",
    "    if a.ndim < 2:\n",
    "        a = [a]\n",
    "    if show_head:\n",
    "        display(HTML(tabulate.tabulate(a[:5], tablefmt='html')))\n",
    "        return\n",
    "    display(HTML(tabulate.tabulate(a, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f8d1b4b0fa08033a72fcaa188b29516",
     "grade": false,
     "grade_id": "cell-9f7d286c15b1e9ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nose.tools as test_\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a24cb340a2b643d5c6136fc9b8f73e9c",
     "grade": false,
     "grade_id": "cell-333c2f053e3e69a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the csv file dataset into a dataframe\n",
    "tweets_df = pd.read_csv(\"UK_MPs_tweets/MPsTweets_from_24Aug_31Aug_2020.csv\")\n",
    "len(tweets_df) # total tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b97c4904def1c4389d7857d2cd01a3e7",
     "grade": false,
     "grade_id": "cell-d9fde8d0ae0d440b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# You may define any helper functions in this cell or any other cell if needed.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e293b6154b1c1613093691e5b23aeec6",
     "grade": false,
     "grade_id": "cell-a51365832d0514fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "In the function below, given a text, return preprocessed text such that:\n",
    "- it is tokenized.\n",
    "- it only contains unicode chars (alphabets, digits or underscore only).\n",
    "- Remove trailing 's e.g. `teacher's` becomes `teacher`\n",
    "- apostrophe is removed. e.g. `don't` becomes `dont`\n",
    "- tokens are lemmatized\n",
    "- Contains no stopwords if list of stopwords is provided.\n",
    "- Extract words from hashtag. `#BackToSchool` becomes `back, to, school` (if stopwords list is empty otherwise `to` will also be removed)\n",
    "- each token is lowercased.\n",
    "- All valid `t.co` URLs are removed. \n",
    "\n",
    "`Hints:`\n",
    "- For hashtags, each word starts with a capital letter; Assume that it's always true.\n",
    "- For URLs, pay attention to length of a valid `t.co` URL.\n",
    "- If the above preprocessing points are applied in the right order, solving this problem is simpler.\n",
    "\n",
    "\n",
    "`More Hints/Pointers with examples are in the test cases below; Comments are also written for clarifications.`\n",
    "\n",
    "`A Hint from Twitter:` \n",
    "'You cannot add spaces or punctuation in a hashtag, or it will not work properly. (Twitter)'\n",
    "\n",
    "\n",
    "`Useful Libraries:`\n",
    "1. [NLTK](https://www.nltk.org)\n",
    "2. [Regular Expressions](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ddba6315b2191faed5a37b34479a7bca",
     "grade": false,
     "grade_id": "cell-393c3501c27f3263",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create class (instead of a function) so that we don't have to pass \n",
    "# stopwords in every func call\n",
    "\n",
    "class PreprocessTweets(object): \n",
    "    \n",
    "    def __init__(self, _stopwords=[]):\n",
    "        self.stopwords = _stopwords\n",
    "        \n",
    "    def __call__(self, tweet_text): # call this everytime an object of this class is instantiated\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77bc869b1484a2b646bdd71dc19c054d",
     "grade": true,
     "grade_id": "cell-e458e596b7854e3b",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"  >>>>> Testing Without Stopwords <<<<< \"\n",
    "#___________________________________________________\n",
    "preprocess = PreprocessTweets()\n",
    "test_.eq_ ((preprocess(\"teacher's\",)), ['teacher'])\n",
    "test_.eq_ ((preprocess(\"Let's get them back to classroom\",)), ['let', 'get', 'them', 'back', 'to', 'classroom'])\n",
    "\n",
    "# Underscore is a unicode char. It should be in the output\n",
    "test_.eq_ ((preprocess(\"@gone_too_far__ Read the article \",)), \n",
    "           ['gone_too_far__', 'read', 'the', 'article'])\n",
    "\n",
    "test_.eq_ ((preprocess(\"Good luckðŸ‘ðŸ‘,Alex_Stafford\")), ['good', 'luck', 'alex_stafford'])\n",
    "test_.eq_  (preprocess(\"$%#hello-^^world!!\"), ['world', 'hello'])\n",
    "#___________________________________________________\n",
    "\n",
    "# URL \n",
    "test_.eq_ (preprocess(\"Register ðŸ‘‰ https://t.co/dCjWFDKoKO\"), ['register'])\n",
    "# There is text embedded after the URL. Extract it.\n",
    "test_.eq_ (preprocess(\"https://t.co/MAdn2K1PwH,Alex_Stafford,Conservative\"), \\\n",
    "           ['alex_stafford', 'conservative'])\n",
    "test_.eq_ (preprocess(\"Register ðŸ‘‰ https://t.co/3zi5fXSrOkHello-Griffitha,Conservative\"), \\\n",
    "           ['register', 'hello', 'griffitha', 'conservative'])\n",
    "\n",
    "# URL contains non-unicode chars: Invalid URL. Don't Remove it entirely.\n",
    "test_.eq_ (preprocess(\"https://t.co/ZhEyÃ„Â¶aaaa\"), ['http', 't', 'co', 'zhey', 'aaaa'])\n",
    "\n",
    "test_.eq_ ((preprocess(\"I'm live now with @AngelaRayner of @UKLabour as I\")), \n",
    "           ['im', 'live', 'now', 'with', 'angelarayner', 'of', 'uklabour', 'a', 'i'])\n",
    "\n",
    "test_.eq_ (preprocess(\"Don'tâ€¦',GwynneMP,Labour\"), ['dont', 'gwynnemp', 'labour'])\n",
    "\n",
    "#___________________________________________________\n",
    "\n",
    "# Hashtags Split at words. For simplicity, \n",
    "# assume that next word starts with a capital letter \n",
    "\n",
    "test_.eq_ (preprocess(' #ShopLocal'), ['shop', 'local'])\n",
    "test_.eq_ (preprocess(' #EatOutHelpOut'), ['eat', 'out', 'help', 'out'])\n",
    "test_.eq_ (preprocess('#InternationalDayoftheDisappeared'),\n",
    "           ['international', 'dayofthe', 'disappeared'])\n",
    "'''\n",
    "\n",
    "'Aside':  # It's possible to split the above example into:\n",
    "['International', 'Day', 'of', 'the', 'Disappeared'] using probablistic models. \n",
    "We' won't ask you to do so for the sake of this test- primarily for simplicity.\n",
    "If interested, check this small module out >>> pip install wordninja\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\"  >>>>> Testing With Stopwords <<<<< \"\n",
    "\n",
    "#___________________________________________________\n",
    "\n",
    "preprocess_with_stopwords = PreprocessTweets(stopwords)\n",
    "test_.eq_ ((preprocess_with_stopwords(\"LET'S get them back to classroom\")), \n",
    "           ['let', 'get', 'back', 'classroom'])\n",
    "\n",
    "test_.eq_ ((preprocess_with_stopwords(\"I'm live now with @AngelaRayner of @UKLabour as I\")), \n",
    "                                ['im', 'live', 'angelarayner', 'uklabour'])\n",
    "# https etc are stopwords. Now they'll be removed.\n",
    "test_.eq_ (preprocess_with_stopwords(\"https://t.co/ZhEyÃ„Â¶cccc\"), ['zhey', 'cccc'])\n",
    "\n",
    "test_.eq_ ((preprocess_with_stopwords('https://t.co/vVzR52faue\",Afzal4Gorton,Labour')), \n",
    "                    ['afzal4gorton', 'labour'])\n",
    "#___________________________________________________\n",
    "\n",
    "test_.eq_ (\n",
    "    preprocess_with_stopwords\n",
    "    ( # Don't confuse: backslashes are to break the line (not part of the tweet text)\n",
    "        \"âœˆ It's back to London today for a new Parliamentary\\\n",
    "        SessionðŸ—“ This Week I'mðŸ‘¨â€âš•ï¸ In Health QuestionsðŸŸ  \\\n",
    "        In the Fisheries Bill DebateðŸ™‹â€â™‚ï¸ Question Number     \\\n",
    "        One in PMQsó ¢ó ³ó £ó ´ó ¿ Talking more Fish in Scottish Affairs       \\\n",
    "        Committee ðŸš¢ Raising issue of Freedom of Navigation on   \\\n",
    "        the South China Sea https://t.co/aisLmptsCR\"\n",
    "    ), \n",
    "    [\n",
    "        'back',\n",
    "        'london',\n",
    "        'today',\n",
    "        'new',\n",
    "        'parliamentary',\n",
    "        'session',\n",
    "        'week',\n",
    "        'im',\n",
    "        'health',\n",
    "        'question',\n",
    "        'fishery',\n",
    "        'bill',\n",
    "        'debate',\n",
    "        'question',\n",
    "        'number',\n",
    "        'one',\n",
    "        'pmqs',\n",
    "        'talking',\n",
    "        'fish',\n",
    "        'scottish',\n",
    "        'affair',\n",
    "        'committee',\n",
    "        'raising',\n",
    "        'issue',\n",
    "        'freedom',\n",
    "        'navigation',\n",
    "        'south',\n",
    "        'china',\n",
    "        'sea'\n",
    "    ]\n",
    ")\n",
    "#___________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ade4f3c53db884f257399cf985a81d0",
     "grade": false,
     "grade_id": "cell-7645ce04b7456494",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract tweets text (raw features) and labels.\n",
    "raw_features_tweets = tweets_df['tweets']\n",
    "labels = tweets_df['Party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b5664a18d79b103337fcb35fcf50e1a2",
     "grade": false,
     "grade_id": "cell-b57602a35a0baa3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess features using preprocess.\n",
    "preprocessed_features = raw_features_tweets.apply(func=lambda tweet_text: preprocess(tweet_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "86eddfd414951aa66c05444c70808fcb",
     "grade": false,
     "grade_id": "cell-e2baddb12a757281",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Put preprocessed features and labels together again.\n",
    "preprocessed_df = pd.concat([preprocessed_features, labels], axis=1)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7276578ad4901c279952a8dd5f9b40f6",
     "grade": false,
     "grade_id": "cell-36224999e4a333c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Split into train/test dataset.\n",
    "train_df, test_df = train_test_split(preprocessed_df, test_size=0.15, \n",
    "                                   random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05aa093c8be6d22c0091f6c2e600a281",
     "grade": false,
     "grade_id": "cell-3a651fbf4b3f26e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is how train set looks like.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features (TFIDF Vectorization)\n",
    "### Question \n",
    "Create TFIDF matrix using [TfidfVectorizer of sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "1. [`Hint`](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/)\n",
    "2. [`Documentation elaborating the hint`](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bca1fd2d21650825d23cff25df2dca81",
     "grade": false,
     "grade_id": "cell-db0c49f6ae3dcc64",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_TFIDF_vectorizer(train_data):\n",
    "    '''\n",
    "    args: train_data -> pandas.core.series.Series\n",
    "    return: Fitted (not transformed) TFIDF Vectorizer -> i.e.\n",
    "                        (sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d0737cf51305bb78633d177f53ee956",
     "grade": false,
     "grade_id": "cell-249a0891d7a4bf54",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract Train tweets text\n",
    "train_corpus = train_df['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2691be82a7e9c93d49b45ad3a3a8d16",
     "grade": true,
     "grade_id": "cell-576ca6e291865a31",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf__ = fit_TFIDF_vectorizer(train_corpus)\n",
    "X_train__ = tfidf__.transform(train_corpus)\n",
    "test_.ok_ ((X_train__.nonzero()[:5][0][-15:] == np.asarray(\n",
    "    [\n",
    "        2942, 2942, 2942, 2942, 2942, 2942, \n",
    "        2942, 2942, 2942, 2942, 2943,\n",
    "        2943, 2943, 2943, 2943\n",
    "    ]\n",
    ")).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aed7364eea69096c706b919de870b37",
     "grade": false,
     "grade_id": "cell-f8afa649b50c9333",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tfidf = fit_TFIDF_vectorizer(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c773a66ed814d5c5e33e233f2e24869",
     "grade": false,
     "grade_id": "cell-1a65f44f076af521",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# In the transform method, actual TFIDF matrix will be created.\n",
    "X_train = tfidf.transform(train_corpus)\n",
    "# Convert labels to integers [0,1].\n",
    "Train_labels = train_df['Party']\n",
    "y_train = [1 if l == 'Conservative' else 0 for l in Train_labels]\n",
    "y_train[:5] # First 5 labels. 1 for conservative. 0 for labour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e2cd5203e8a68dbf01c6d367a0992d4",
     "grade": false,
     "grade_id": "cell-4e6e03dbbe6f5afa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a SVC.\n",
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "225556dd0235b98bdb9823742e06e101",
     "grade": false,
     "grade_id": "cell-87101cb9427bd1cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the classifier on train data.\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e7483f2222bb7fceb752273308e2c6f",
     "grade": false,
     "grade_id": "cell-e6512d42a7d4bc0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Performance (R2 score) on train data.\n",
    "clf.score(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e993d2725e67658ab3aa2b0f6dc4405e",
     "grade": false,
     "grade_id": "cell-3e30235930b1a67a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Make TFIDF features of test data.\n",
    "test_corpus = test_df['tweets']\n",
    "X_test = tfidf.transform(test_corpus)\n",
    "Test_labels = test_df['Party']\n",
    "y_test = [1 if l == 'Conservative' else 0 for l in Test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1bbf48f327af24a0db0432f3d7519827",
     "grade": false,
     "grade_id": "cell-8ad068044071010f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Performance (R2 score) on test data.\n",
    "clf.score(X_test, y_test) # Highest possible = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2811d3a709b498674386a31d01dce0b7",
     "grade": false,
     "grade_id": "cell-1b4ee4e31b58791b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40dbefe774cdd8de54e034e3145dda9d",
     "grade": false,
     "grade_id": "cell-ffe162c26621a7f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy on test data.\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Remarks:`\n",
    "The classifier, for example, may learn that if a tweet mentions something negative about Boris Jhonson ('Conservative' party ), then it's more likely the author of that tweet is `not` of 'Conservative' party. It learned all that from the training examples with labels we provided. And the classifier's prediction is correct about $~82\\%$ of the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
